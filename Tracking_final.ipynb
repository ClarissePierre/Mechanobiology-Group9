{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "# for file handling\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import shutil\n",
    "# reading files under given folder\n",
    "import glob\n",
    "# progress bar\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot\n",
    "# imaging processing\n",
    "import skimage as ski\n",
    "# data processing and manipulation\n",
    "import pandas as pd\n",
    "import skimage as ski\n",
    "import tifffile\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import trackpy as tp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking for a dilute condition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the dataframe with all bacteria metadata. In the dataframe that we load, we have indices for the time (frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diluted one\n",
    "#df_track=pd.read_csv(\"/Users/Clarisse/Downloads/spot_metadata_dilute_500_50_tp.csv\")\n",
    "# dense oneÒ\n",
    "df_track=pd.read_csv('/Users/Clarisse/Single-bacterium-tracking/data/pilG_dense_PC_metadata_800_800.csv')\n",
    "df_track.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_track.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use the link function to assign each particle found in each dataframe to a certain bacteria. This hides a loss function: \n",
    "\n",
    "\"The tracking algorithm, at its simplest level, takes each particle in the previous frame and tries to find it in the current frame. This requires knowing where to look for it; if we find an actual particle near that spot, it's probably a match. The basic algorithm (Crocker & Grier) was developed to track particles undergoing Brownian diffusion, which ideally means that a particle's velocity is uncorrelated from one frame to the next. Therefore, the best guess for where a particle is going is that it will be near its most recent location.\" (trackpy documentation)\n",
    "\n",
    "We choose the average size of the bacteria, here 4 pixels, to optimize the linking function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=tp.link_df(df_track,4)\n",
    "t1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at what the trajectories look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tp.plot_traj(t1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several long lasting trajectories as well as some single points. You can see in the following cell that the linking algorithm has found more than 900 different trajectories in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.particle.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this issue, we use filter_stubs to get rid of spurious trajectories. We choose a parameter (minimum number of points to be considered) that allows us to be as close as possible to the number of bacteria in the initial frame (118)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tp.filter_stubs(t1, 10)\n",
    "# Compare the number of particles in the unfiltered and filtered data.\n",
    "print('Before:', t1['particle'].nunique())\n",
    "print('After:', t2['particle'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tp.plot_traj(t2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see way less trajectories made only of one point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter that could interfer with our analysis would be drifting. We look at the drift in both x and y dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tp.compute_drift(t2)\n",
    "d.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of this drift, we use the dedicated function, substract_drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tm = tp.subtract_drift(t2.copy(), d)\n",
    "ax = tp.plot_traj(tm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(not sure it's needed, usually used for small particles, Brownian motion)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean Squared Displacement of Individal Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = tp.imsd(tm, 100/285., 24)  # microns per pixel = 100/285., frames per second = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(im.index, im, 'k-', alpha=0.1)  # black lines, semitransparent\n",
    "ax.set(ylabel=r'$\\langle \\Delta r^2 \\rangle$ [$\\mu$m$^2$]',\n",
    "       xlabel='lag time $t$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ensemble Mean Squared Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = tp.emsd(tm, 100/285., 1) # microns per pixel = 100/285., frames per second = 24\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(em.index, em, 'o')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set(ylabel=r'$\\langle \\Delta r^2 \\rangle$ [$\\mu$m$^2$]',\n",
    "       xlabel='lag time $t$')\n",
    "ax.set(ylim=(1e-2, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(r'$\\langle \\Delta r^2 \\rangle$ [$\\mu$m$^2$]')\n",
    "plt.xlabel('lag time $t$');\n",
    "tp.utils.fit_powerlaw(em)  # performs linear best fit in log space, plots]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting metrics from the tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several metrics we can extract and look at: \n",
    " - the velocity of particles, \n",
    " - the mean orientation, the nematic order \n",
    " - and the number or reversals (or tumbles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nematic order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nematic order is usually used to describe \"the orientational order of the most general biaxial nematic liquid crystal\" (Wiki). It is defined in the paper \"Large-scale orientational order in bacterial colonies during inward growth, Mustafa Basaran\" et al. (2022) as follows:\n",
    "\n",
    "⟨SR⟩=N1∑i cos[2(θi−φi)]\n",
    "\n",
    "Where θi is the angular orientation with respect to x-axis and φi is the angular position of the bacterium i in polar coordinates about the colony center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define theta and phi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta(data, i):\n",
    "    if i < len(data):\n",
    "        theta_i = data['orientation'].iloc[i]\n",
    "        return theta_i\n",
    "    else:\n",
    "        return 0  # Return a default value if the index is out of range\n",
    "\n",
    "def phi(data, i):\n",
    "    if i < len(data):\n",
    "        center = [data.x.mean(), data.y.mean()]\n",
    "        delta_center = [data.x.iloc[i] - center[0], data.y.iloc[i] - center[1]]\n",
    "        phi_i = np.arctan2(delta_center[1], delta_center[0])\n",
    "        return phi_i\n",
    "    else:\n",
    "        return 0  # Return a default value if the index is out of range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also define the nematic order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nematic_order_over_time(tm_cond,fra):\n",
    "    SR = []\n",
    "\n",
    "    for t in fra:\n",
    "        sub_t = tm_cond[tm_cond.frame == t]\n",
    "\n",
    "        sr = 0  # Initialize sr for this time step\n",
    "\n",
    "        for i in range(len(sub_t)):\n",
    "            sr += np.cos(2 * (theta(sub_t, i) - phi(sub_t, i)))\n",
    "\n",
    "        SR.append(sr)\n",
    "\n",
    "    return SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one condition\n",
    "\n",
    "time=[t for t in range (0,np.max(df_track.frame)+1)]\n",
    "\n",
    "N=nematic_order_over_time(tm)\n",
    "N\n",
    "plt.plot(time,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear and angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def velocity(df):\n",
    "\n",
    "    vel = []\n",
    "    ori = []\n",
    "\n",
    "    for part in df.particle.unique():\n",
    "        tm_part = df[df.particle == part].copy()  # create sub-dataframe for each particle\n",
    "        tm_part = tm_part.reset_index(drop=True)\n",
    "\n",
    "        S = 0  # sum of distances traveled\n",
    "        O = 0  # sum of orientation changes\n",
    "\n",
    "        prev_x, prev_y, prev_frame = None, None, None  # Initialize previous position and frame\n",
    "\n",
    "        for index, row in tm_part.iterrows():\n",
    "            x, y, frame = row['x'], row['y'], row['frame']\n",
    "\n",
    "            if prev_x is not None and prev_y is not None and prev_frame is not None:\n",
    "                dist = np.sqrt((x - prev_x) ** 2 + (y - prev_y) ** 2)\n",
    "                S += dist\n",
    "\n",
    "                # Calculate orientation change\n",
    "                O += np.abs(row['orientation'] - tm_part.loc[index - 1, 'orientation'])\n",
    "\n",
    "            prev_x, prev_y, prev_frame = x, y, frame\n",
    "\n",
    "        # Calculate the time elapsed for this particle\n",
    "        total_time = tm_part['frame'].max() - tm_part['frame'].min()\n",
    "\n",
    "        # Calculate mean velocity and mean orientation change considering time\n",
    "        if total_time > 0:\n",
    "            vel.append(S / total_time)\n",
    "            ori.append(O / total_time)\n",
    "        else:\n",
    "            vel.append(0)  # Avoid division by zero\n",
    "            ori.append(0)\n",
    "\n",
    "    \n",
    "    return [vel,ori] # gives a list containing the lists of mean linear and angular velocities of each particle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity(tm)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(file,condition):\n",
    "\n",
    "    #read the file, segmentation data\n",
    "    data=pd.read_csv(file)\n",
    "\n",
    "    # linking\n",
    "    t1=tp.link_df(data,4)\n",
    "\n",
    "    # filtering\n",
    "    t2 = tp.filter_stubs(t1, 10)\n",
    "\n",
    "    # removing drift\n",
    "    d = tp.compute_drift(t2)\n",
    "    tm = tp.subtract_drift(t2.copy(), d)\n",
    "\n",
    "    # get metrics from the data\n",
    "    print('getting metrics')\n",
    "    frames=data.frame.unique()\n",
    "\n",
    "    [vel,ori]=velocity(tm)\n",
    "\n",
    "    part=tm.particle.unique()\n",
    "    \n",
    "    #frames=[t for t in range (0,50)]\n",
    "    # get indices of the bacteria considered\n",
    "\n",
    "    # create a DataFrame with all the metrics calculated\n",
    "    print('creating DataFrames')\n",
    "    table=pd.DataFrame({'particle' : part,\n",
    "                                'velocity' : vel,\n",
    "                                'orientation' : ori }, \n",
    "                                columns=['particle','velocity', 'orientation'])\n",
    "\n",
    "    # save metrics in an excel file\n",
    "    print('saving data to csv')\n",
    "    \n",
    "    name=condition + '_metrics.csv'\n",
    "\n",
    "    table.to_csv('/Users/Clarisse/Single-bacterium-tracking/Data/'+name)\n",
    "\n",
    "    #return(table)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the processing function for all conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Check the processing on diluted files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing(\"/Users/Clarisse/Downloads/spot_metadata_dilute_500_50_tp.csv\",'WT_dilute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Check the processing on dense files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing('/Users/Clarisse/Single-bacterium-tracking/data/pilG_dense_PC_metadata_800_800.csv','WT_dense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing every videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be repeating the same procedure for all of our dataset (i.e. very videos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_big = os.getcwd() + \"\\\\data\\\\Real\\\\big_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big=pd.read_csv(PATH_big)\n",
    "df_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to swap our x and y values and reverse our y axis values (due to the segmentation algorithm method) to be consistent with our videos\n",
    "df_big = df_big.rename({\"x\" : \"y\", \"y\" : \"x\"}, axis=1)\n",
    "#df_big.y = np.abs(df_big.y - 800)\n",
    "df_big.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.filename.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be splicing our big dataframe to track each video separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the filenames\n",
    "uniqueFiles = df_big.filename.unique()\n",
    "\n",
    "#create a data frame dictionary to store our data frames\n",
    "DataFrameDict = {elem : pd.DataFrame() for elem in uniqueFiles}\n",
    "\n",
    "for key in DataFrameDict.keys():\n",
    "    DataFrameDict[key] = df_big[:][df_big.filename == key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To access one of our dataframe we can then just use : DataFrameDict[\"name_of_file\"]\n",
    "DataFrameDict[\"WT_dense_PC\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the tracking procedure, we will just reuse the steps we showed before :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrackingDict = {}\n",
    "\n",
    "for key in DataFrameDict.keys() :\n",
    "    t1 = tp.link_df(DataFrameDict[key],4)\n",
    "    t2 = tp.filter_stubs(t1,10)\n",
    "    d = tp.compute_drift(t2)\n",
    "    tm = tp.subtract_drift(t2.copy(), d)\n",
    "    TrackingDict[key] = tm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to superimpose our trajectories with our raw images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to load the images \n",
    "PATH_IMG = os.getcwd() + \"\\\\data\\\\Real\\\\output\"\n",
    "# glob the file names in the data directory, keep only the file name not the whole path\n",
    "DATA_LIST = [os.path.basename(x) for x in glob.glob(os.path.join(PATH_IMG, '*raw_image*'))]\n",
    "# put DATA_LIST into a pandas dataframe\n",
    "IMG_DF = pd.DataFrame(DATA_LIST, columns=['filename'])\n",
    "display(IMG_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access the file names we use :\n",
    "IMG_DF[\"filename\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = os.getcwd() + \"\\\\data\\\\Result\"\n",
    "DATA_DIR = os.getcwd() + \"\\\\data\\\\Real\\\\output\"\n",
    "\n",
    "for key in TrackingDict.keys() :\n",
    "    # we first create a folder for each of our video\n",
    "    dir = os.path.join(RESULT_DIR,f\"{key}\")\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "    \n",
    "    raw = ski.io.imread(os.path.join(DATA_DIR, f\"{key}_raw_image_800_800.tif\")) # we open our raw cropped images\n",
    "    \n",
    "    for i in range(TrackingDict[key].frame.max()) :# as we want to iterate on every frame\n",
    "        if i == 0 : #There's no tracking on the first frame so we use the raw image\n",
    "            ax = plt.imshow(raw[0],cmap=plt.cm.gray)\n",
    "            ax.get_figure().savefig(dir + f\"\\\\{key}_f{i+1}.png\", format=\"png\")\n",
    "            ax.get_figure().clear() # we clear the current RAM\n",
    "            plt.close()\n",
    "            \n",
    "        else : #all the other frames\n",
    "            t1 = TrackingDict[key]\n",
    "            ax = tp.plot_traj(traj=t1[t1.frame<=i],superimpose=raw[i]) # get our raw impose with the tracking superimposed on top\n",
    "            ax.get_figure().savefig(dir + f\"\\\\{key}_f{i+1}.png\", format=\"png\")\n",
    "            ax.get_figure().clear()\n",
    "            plt.close() # we clear the current RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now we generate a tif file with all of our pngs :\n",
    "RESULT_DIR = os.getcwd() + \"\\\\data\\\\Result\"\n",
    "DATA_DIR = os.getcwd() + \"\\\\data\\\\Real\\\\output\"\n",
    "\n",
    "for key in TrackingDict.keys() :\n",
    "    img = []\n",
    "    for i in range(TrackingDict[key].frame.max()) :# as we want to iterate on every frame\n",
    "        img.append(mpimg.imread(RESULT_DIR + f\"\\\\{key}\" + f\"\\\\{key}_f{i+1}.png\"))\n",
    "    \n",
    "    ski.io.imsave(os.path.join(RESULT_DIR + f\"\\\\{key}\", f'{key}.tif'), np.array(img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now get the velocities and orientations for every videos :\n",
    "for key in TrackingDict.keys() :\n",
    "    frames = TrackingDict[key].frame.unique()\n",
    "    [vel,ori]=velocity(TrackingDict[key])\n",
    "    part = TrackingDict[key].particle.unique()\n",
    "    \n",
    "    # create a DataFrame with all the metrics calculated\n",
    "    print('creating DataFrames')\n",
    "    table=pd.DataFrame({'particle' : part,\n",
    "                                'velocity' : vel,\n",
    "                                'orientation' : ori }, \n",
    "                                columns=['particle','velocity', 'orientation'])\n",
    "\n",
    "    # save metrics in an excel file\n",
    "    print('saving data to csv')\n",
    "    \n",
    "    table.to_csv(os.path.join(RESULT_DIR + f\"\\\\{key}\", f'{key}_metrics.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the different metrics for different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for linear velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for angular velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot for number of reversals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Superposed lineplots for nematic order over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
